{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMo5rTsgxKDY8YvFXsYBnGc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nadavo11/Neural_Networks/blob/master/Linear%20regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uDixv-8x9GR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets, linear_model, metrics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "yYGhgYGwyJMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overview:\n",
        "\n",
        "To predict the progression of diabetes in patients, we will use linear regression with gradient descent in this hands-on assignment. In this tutorial, you will learn how to implement linear regression with gradient descent in Python.\n",
        "\n",
        "Using scikit-learn, a Python machine learning library, we will first load and train a linear regression model. Our implementation will be tested against the results of scikit-learn. Our next step will be to implement linear regression using gradient descent."
      ],
      "metadata": {
        "id": "ZcKkKu9xyOVM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load diabetes dataset\n",
        "diabetes = datasets.load_diabetes()\n",
        "diabetes_X = diabetes.data  # matrix of dimensions 442x10\n",
        "\n",
        "# Split the data into training/testing sets\n",
        "diabetes_X_train = diabetes_X[:-20]\n",
        "diabetes_X_test = diabetes_X[-20:]\n",
        "\n",
        "# Split the targets into training/testing sets\n",
        "diabetes_y_train = diabetes.target[:-20]\n",
        "diabetes_y_test = diabetes.target[-20:]"
      ],
      "metadata": {
        "id": "P3xMx7zxyRBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data description:\n",
        "A total of 442 diabetes patients are included in the dataset. There are 10 input variables for each patient - age, sex, body mass index, average blood pressure, and six measurements of blood serum. The blood serum measurements are: Total Cholesterol (TC), Low Density Lipoprotein (LDL), High Density Lipoprotein (HDL), TC/HDL, Low Tension Glaucoma (LTG) and Glucose. The target is a quantitative measure of disease progression after one year."
      ],
      "metadata": {
        "id": "Ro70u_M6yasu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sanity check:\n",
        "In order to determine whether our implementation is correct, we will use the results from scikit-learn. The linear regression model in scikit-learn can be trained with just a call to function fit on the model since scikit-learn is a machine learning library. You can see the documentation here."
      ],
      "metadata": {
        "id": "JQytydhWyixk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# with scikit learn:\n",
        "# Create linear regression object\n",
        "regr = linear_model.LinearRegression()\n",
        "\n",
        "# Train the model using the training sets\n",
        "regr.fit(diabetes_X_train, diabetes_y_train)\n",
        "\n",
        "# Make predictions using the testing set\n",
        "diabetes_y_pred = regr.predict(diabetes_X_test)\n",
        "\n",
        "# The coefficients\n",
        "print(\"Coefficients: \\n\", regr.coef_)\n",
        "\n",
        "# The mean squared error\n",
        "mean_squared_error = metrics.mean_squared_error(diabetes_y_test, diabetes_y_pred)\n",
        "print(\"Mean squared error: %.2f\" % mean_squared_error)\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "0vXHfugPymMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing Linear Regression with Gradient Descent\n",
        "Finally, it's time to implement linear regression ourselves."
      ],
      "metadata": {
        "id": "iG6x4ztkyu_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train\n",
        "X = diabetes_X_train\n",
        "y = diabetes_y_train\n",
        "\n",
        "# train: init\n",
        "n = len(X)\n",
        "\n",
        "W = np.random.rand(len(X[0]))\n",
        "b = np.random.rand()\n",
        "\n",
        "learning_rate = 0.8\n",
        "epochs = 1000000\n",
        "\n",
        "\n",
        "# train: gradient descent\n",
        "for i in range(epochs):\n",
        "    if i % 100000 == 0:\n",
        "      learning_rate = 1/4 + (1/4)*100000/epochs\n",
        "    # calculate predictions\n",
        "    prediction = np.array([np.dot(x, W) + b for x in X])\n",
        "    # TODO\n",
        "\n",
        "    # calculate error and cost (mean squared error)\n",
        "    mean_squared_error = metrics.mean_squared_error(y,prediction)\n",
        "    errors = (y- prediction)\n",
        "    cost = np.sqrt(mean_squared_error)\n",
        "    # TODO\n",
        "\n",
        "    # calculate gradients\n",
        "\n",
        "    Derivative_by_w = (2/n)*np.dot(errors,X)\n",
        "    Derivative_by_b = 2*np.mean(errors)\n",
        "    # TODO  \n",
        "\n",
        "    # update parameters\n",
        "    W += learning_rate*Derivative_by_w\n",
        "    b += learning_rate*Derivative_by_b\n",
        "    # TODO \n",
        "\n",
        "    # diagnostic output\n",
        "    if i % 5000 == 0:\n",
        "        print(\"Epoch %d: %f\" % (i, mean_squared_error))\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzuXkGkMy0XF",
        "outputId": "a83bf0d7-16d6-42b0-db57-6ddfad2c2c78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: 29256.911968\n",
            "Epoch 5000: 2923.460951\n",
            "Epoch 10000: 2920.938790\n",
            "Epoch 15000: 2918.862465\n",
            "Epoch 20000: 2917.044259\n",
            "Epoch 25000: 2915.430642\n",
            "Epoch 30000: 2913.990304\n",
            "Epoch 35000: 2912.701460\n",
            "Epoch 40000: 2911.546968\n",
            "Epoch 45000: 2910.512361\n",
            "Epoch 50000: 2909.585017\n",
            "Epoch 55000: 2908.753749\n",
            "Epoch 60000: 2908.008577\n",
            "Epoch 65000: 2907.340575\n",
            "Epoch 70000: 2906.741748\n",
            "Epoch 75000: 2906.204931\n",
            "Epoch 80000: 2905.723702\n",
            "Epoch 85000: 2905.292304\n",
            "Epoch 90000: 2904.905578\n",
            "Epoch 95000: 2904.558898\n",
            "Epoch 100000: 2904.248117\n",
            "Epoch 105000: 2903.969517\n",
            "Epoch 110000: 2903.719767\n",
            "Epoch 115000: 2903.495878\n",
            "Epoch 120000: 2903.295173\n",
            "Epoch 125000: 2903.115251\n",
            "Epoch 130000: 2902.953961\n",
            "Epoch 135000: 2902.809372\n",
            "Epoch 140000: 2902.679755\n",
            "Epoch 145000: 2902.563560\n",
            "Epoch 150000: 2902.459397\n",
            "Epoch 155000: 2902.366021\n",
            "Epoch 160000: 2902.282313\n",
            "Epoch 165000: 2902.207274\n",
            "Epoch 170000: 2902.140005\n",
            "Epoch 175000: 2902.079701\n",
            "Epoch 180000: 2902.025642\n",
            "Epoch 185000: 2901.977181\n",
            "Epoch 190000: 2901.933738\n",
            "Epoch 195000: 2901.894794\n",
            "Epoch 200000: 2901.859882\n",
            "Epoch 205000: 2901.828586\n",
            "Epoch 210000: 2901.800530\n",
            "Epoch 215000: 2901.775379\n",
            "Epoch 220000: 2901.752833\n",
            "Epoch 225000: 2901.732622\n",
            "Epoch 230000: 2901.714503\n",
            "Epoch 235000: 2901.698261\n",
            "Epoch 240000: 2901.683700\n",
            "Epoch 245000: 2901.670647\n",
            "Epoch 250000: 2901.658946\n",
            "Epoch 255000: 2901.648457\n",
            "Epoch 260000: 2901.639053\n",
            "Epoch 265000: 2901.630624\n",
            "Epoch 270000: 2901.623067\n",
            "Epoch 275000: 2901.616293\n",
            "Epoch 280000: 2901.610220\n",
            "Epoch 285000: 2901.604776\n",
            "Epoch 290000: 2901.599896\n",
            "Epoch 295000: 2901.595521\n",
            "Epoch 300000: 2901.591600\n",
            "Epoch 305000: 2901.588084\n",
            "Epoch 310000: 2901.584932\n",
            "Epoch 315000: 2901.582107\n",
            "Epoch 320000: 2901.579574\n",
            "Epoch 325000: 2901.577304\n",
            "Epoch 330000: 2901.575268\n",
            "Epoch 335000: 2901.573444\n",
            "Epoch 340000: 2901.571808\n",
            "Epoch 345000: 2901.570342\n",
            "Epoch 350000: 2901.569027\n",
            "Epoch 355000: 2901.567849\n",
            "Epoch 360000: 2901.566793\n",
            "Epoch 365000: 2901.565846\n",
            "Epoch 370000: 2901.564997\n",
            "Epoch 375000: 2901.564236\n",
            "Epoch 380000: 2901.563554\n",
            "Epoch 385000: 2901.562942\n",
            "Epoch 390000: 2901.562394\n",
            "Epoch 395000: 2901.561903\n",
            "Epoch 400000: 2901.561462\n",
            "Epoch 405000: 2901.561067\n",
            "Epoch 410000: 2901.560713\n",
            "Epoch 415000: 2901.560396\n",
            "Epoch 420000: 2901.560111\n",
            "Epoch 425000: 2901.559856\n",
            "Epoch 430000: 2901.559627\n",
            "Epoch 435000: 2901.559422\n",
            "Epoch 440000: 2901.559239\n",
            "Epoch 445000: 2901.559074\n",
            "Epoch 450000: 2901.558926\n",
            "Epoch 455000: 2901.558794\n",
            "Epoch 460000: 2901.558675\n",
            "Epoch 465000: 2901.558569\n",
            "Epoch 470000: 2901.558474\n",
            "Epoch 475000: 2901.558388\n",
            "Epoch 480000: 2901.558311\n",
            "Epoch 485000: 2901.558243\n",
            "Epoch 490000: 2901.558181\n",
            "Epoch 495000: 2901.558126\n",
            "Epoch 500000: 2901.558076\n",
            "Epoch 505000: 2901.558032\n",
            "Epoch 510000: 2901.557992\n",
            "Epoch 515000: 2901.557957\n",
            "Epoch 520000: 2901.557925\n",
            "Epoch 525000: 2901.557896\n",
            "Epoch 530000: 2901.557870\n",
            "Epoch 535000: 2901.557847\n",
            "Epoch 540000: 2901.557827\n",
            "Epoch 545000: 2901.557808\n",
            "Epoch 550000: 2901.557792\n",
            "Epoch 555000: 2901.557777\n",
            "Epoch 560000: 2901.557763\n",
            "Epoch 565000: 2901.557751\n",
            "Epoch 570000: 2901.557741\n",
            "Epoch 575000: 2901.557731\n",
            "Epoch 580000: 2901.557723\n",
            "Epoch 585000: 2901.557715\n",
            "Epoch 590000: 2901.557708\n",
            "Epoch 595000: 2901.557702\n",
            "Epoch 600000: 2901.557696\n",
            "Epoch 605000: 2901.557691\n",
            "Epoch 610000: 2901.557687\n",
            "Epoch 615000: 2901.557683\n",
            "Epoch 620000: 2901.557679\n",
            "Epoch 625000: 2901.557676\n",
            "Epoch 630000: 2901.557673\n",
            "Epoch 635000: 2901.557670\n",
            "Epoch 640000: 2901.557668\n",
            "Epoch 645000: 2901.557666\n",
            "Epoch 650000: 2901.557664\n",
            "Epoch 655000: 2901.557663\n",
            "Epoch 660000: 2901.557661\n",
            "Epoch 665000: 2901.557660\n",
            "Epoch 670000: 2901.557658\n",
            "Epoch 675000: 2901.557657\n",
            "Epoch 680000: 2901.557656\n",
            "Epoch 685000: 2901.557656\n",
            "Epoch 690000: 2901.557655\n",
            "Epoch 695000: 2901.557654\n",
            "Epoch 700000: 2901.557653\n",
            "Epoch 705000: 2901.557653\n",
            "Epoch 710000: 2901.557652\n",
            "Epoch 715000: 2901.557652\n",
            "Epoch 720000: 2901.557652\n",
            "Epoch 725000: 2901.557651\n",
            "Epoch 730000: 2901.557651\n",
            "Epoch 735000: 2901.557651\n",
            "Epoch 740000: 2901.557650\n",
            "Epoch 745000: 2901.557650\n",
            "Epoch 750000: 2901.557650\n",
            "Epoch 755000: 2901.557650\n",
            "Epoch 760000: 2901.557650\n",
            "Epoch 765000: 2901.557649\n",
            "Epoch 770000: 2901.557649\n",
            "Epoch 775000: 2901.557649\n",
            "Epoch 780000: 2901.557649\n",
            "Epoch 785000: 2901.557649\n",
            "Epoch 790000: 2901.557649\n",
            "Epoch 795000: 2901.557649\n",
            "Epoch 800000: 2901.557649\n",
            "Epoch 805000: 2901.557649\n",
            "Epoch 810000: 2901.557649\n",
            "Epoch 815000: 2901.557648\n",
            "Epoch 820000: 2901.557648\n",
            "Epoch 825000: 2901.557648\n",
            "Epoch 830000: 2901.557648\n",
            "Epoch 835000: 2901.557648\n",
            "Epoch 840000: 2901.557648\n",
            "Epoch 845000: 2901.557648\n",
            "Epoch 850000: 2901.557648\n",
            "Epoch 855000: 2901.557648\n",
            "Epoch 860000: 2901.557648\n",
            "Epoch 865000: 2901.557648\n",
            "Epoch 870000: 2901.557648\n",
            "Epoch 875000: 2901.557648\n",
            "Epoch 880000: 2901.557648\n",
            "Epoch 885000: 2901.557648\n",
            "Epoch 890000: 2901.557648\n",
            "Epoch 895000: 2901.557648\n",
            "Epoch 900000: 2901.557648\n",
            "Epoch 905000: 2901.557648\n",
            "Epoch 910000: 2901.557648\n",
            "Epoch 915000: 2901.557648\n",
            "Epoch 920000: 2901.557648\n",
            "Epoch 925000: 2901.557648\n",
            "Epoch 930000: 2901.557648\n",
            "Epoch 935000: 2901.557648\n",
            "Epoch 940000: 2901.557648\n",
            "Epoch 945000: 2901.557648\n",
            "Epoch 950000: 2901.557648\n",
            "Epoch 955000: 2901.557648\n",
            "Epoch 960000: 2901.557648\n",
            "Epoch 965000: 2901.557648\n",
            "Epoch 970000: 2901.557648\n",
            "Epoch 975000: 2901.557648\n",
            "Epoch 980000: 2901.557648\n",
            "Epoch 985000: 2901.557648\n",
            "Epoch 990000: 2901.557648\n",
            "Epoch 995000: 2901.557648\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing our model\n",
        "\n",
        "now le'ts put our model to the test. will it predict diabities in tested individuals?"
      ],
      "metadata": {
        "id": "ZAA_JFM8DiDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test\n",
        "X = diabetes_X_test\n",
        "y = diabetes_y_test\n",
        "\n",
        "prediction = np.dot(X, W)+b\n",
        "mean_squared_error = metrics.mean_squared_error(y,prediction)\n",
        "\n",
        "print(\"tested mean square error: \" ,( mean_squared_error))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cka5MezzD0gJ",
        "outputId": "56560005-529a-4e9b-83d5-d92eac58bc6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tested mean square error:  2004.5672081086632\n"
          ]
        }
      ]
    }
  ]
}